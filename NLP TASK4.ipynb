{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfDJRgNGJR8CtLEmSKFtdD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grC68DWGKyqI","executionInfo":{"status":"ok","timestamp":1756963575796,"user_tz":-330,"elapsed":163,"user":{"displayName":"MAADHAM REDDYLAKSHMI,CSE(2022) Vel Tech, Chennai","userId":"02155497753756987257"}},"outputId":"fd8542d6-3a66-4385-ad2a-47c57551fbb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Morphology of the document:\n","natural: 2\n","language: 2\n","processing: 2\n","code: 2\n","sample: 1\n","document: 1\n","contains: 1\n","several: 1\n","sentences: 1\n","test: 1\n","tokenize: 1\n","remove: 1\n","stopwords: 1\n","find: 1\n","frequent: 1\n","words: 1\n","fascinating: 1\n","field: 1\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.probability import FreqDist\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab') # Download the missing resource\n","\n","def load_document(file_path):\n","  with open(\"/content/drive/MyDrive/NLP TASKS/NLP TASK 4 DATASET.txt\", 'r', encoding='utf-8') as file:\n","    return file.read()\n","\n","def tokenize_document(document):\n","  tokens = word_tokenize(document)\n","  return [word.lower() for word in tokens if word.isalpha()] # Remove punctuation and convert to lowercase\n","\n","def remove_stopwords(tokens):\n","  stop_words = set(stopwords.words('english'))\n","  return [word for word in tokens if word not in stop_words]\n","\n","def find_morphology(tokens):\n","  fdist = FreqDist(tokens)\n","  return fdist.most_common() # Return the 10 most common words and their frequencies\n","\n","document_path = \"/content/drive/MyDrive/NLP TASKS/NLP TASK 4 DATASET.txt\" # Corrected file path\n","document = load_document(document_path)\n","tokens = tokenize_document(document)\n","tokens_without_stopwords = remove_stopwords(tokens)\n","morphology = find_morphology(tokens_without_stopwords)\n","\n","print(\"Morphology of the document:\")\n","for word, frequency in morphology:\n","  print(f\"{word}: {frequency}\")"]},{"cell_type":"code","source":["# Create a directory if it doesn't exist\n","import os\n","os.makedirs(\"/content/drive/MyDrive/NLP TASKS\", exist_ok=True)\n","\n","# Write the sample data to a file\n","sample_data = \"\"\"This is a sample document for natural language processing. It contains several sentences to test the code. The code will tokenize, remove stopwords, and find the most frequent words. Natural language processing is a fascinating field.\"\"\"\n","\n","with open(\"/content/drive/MyDrive/NLP TASKS/NLP TASK 4 DATASET.txt\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(sample_data)\n","\n","print(\"Sample data file created successfully.\")"],"metadata":{"id":"5OesuopwMHCm","executionInfo":{"status":"ok","timestamp":1756963573832,"user_tz":-330,"elapsed":14,"user":{"displayName":"MAADHAM REDDYLAKSHMI,CSE(2022) Vel Tech, Chennai","userId":"02155497753756987257"}},"outputId":"7f34e3bd-c250-44ca-fd0a-40d83613fe0e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample data file created successfully.\n"]}]}]}