{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONVj5GErLCDZRaz8NvLb/A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95b864dd","executionInfo":{"status":"ok","timestamp":1753943584559,"user_tz":-330,"elapsed":215,"user":{"displayName":"MAADHAM REDDYLAKSHMI,CSE(2022) Vel Tech, Chennai","userId":"02155497753756987257"}},"outputId":"df7067b5-e1ca-43dc-909d-ffce166b7ccc"},"source":["try:\n","    import transformers\n","except ImportError:\n","    print(\"Installing transformers library...\")\n","    !pip install transformers\n","    import transformers\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import re\n","\n","# Download necessary NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","text = \"tokenization without transformers is straightforward with tools like NLTK\"\n","\n","# Tokenization with Transformers\n","print(\"--- Transformers Tokenization ---\")\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","tokens_transformers = tokenizer(text)\n","print(\"transformers tokens:\", tokens_transformers)\n","# Corrected line to convert list of integer IDs to tokens\n","tokens_transformers_list = tokenizer.convert_ids_to_tokens(tokens_transformers['input_ids'][0])\n","print(\"transformers tokens (list):\", tokens_transformers_list)\n","decoded_text = tokenizer.decode(tokens_transformers['input_ids'][0], skip_special_tokens=True)\n","print(\"decoded text:\", decoded_text)\n","\n","# Tokenization with Stopwords as Delimiters\n","print(\"\\n--- Stopwords as Delimiters Tokenization ---\")\n","stop_words = set(stopwords.words('english'))\n","\n","# Create a regex pattern to split on stopwords\n","# Sort stop words by length in descending order to match longer ones first\n","stop_words_sorted = sorted(list(stop_words), key=len, reverse=True)\n","pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in stop_words_sorted) + r')\\b'\n","\n","# Split the text using the stopword pattern\n","tokens_stopwords = re.split(pattern, text, flags=re.IGNORECASE)\n","\n","# Remove empty strings that might result from splitting\n","tokens_stopwords = [token.strip() for token in tokens_stopwords if token   .strip()]\n","\n","print(\"Tokens using stopwords as delimiters:\", tokens_stopwords)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["--- Transformers Tokenization ---\n","transformers tokens: {'input_ids': [101, 19204, 3989, 2302, 19081, 2003, 19647, 2007, 5906, 2066, 17953, 2102, 2243, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","transformers tokens (list): [CLS]\n","decoded text: \n","\n","--- Stopwords as Delimiters Tokenization ---\n","Tokens using stopwords as delimiters: ['tokenization without transformers', 'straightforward', 'tools like NLTK']\n"]}]}]}